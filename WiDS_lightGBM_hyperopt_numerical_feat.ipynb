{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle --upgrade\n",
    "# !pip install lightgbm\n",
    "# !pip install catboost\n",
    "# !pip install dabl\n",
    "# !pip install plotly\n",
    "# !pip install shap\n",
    "# !pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.setup import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wkdir = '/home/jovyan/work/analysis/DATASCI-WiDS'\n",
    "random_state = 33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training =  pd.read_csv(wkdir + \"/data/TrainingWiDS2021.csv\")\n",
    "data_dictionary = pd.read_csv(wkdir + \"/data/DataDictionaryWiDS2021.csv\")\n",
    "training = training.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "unlabeled = pd.read_csv(wkdir + \"/data/UnlabeledWiDS2021.csv\")\n",
    "unlabeled = unlabeled.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = dabl.detect_types(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = data_types[data_types['categorical']==True].index.tolist()\n",
    "print(\"**categorical features**\")\n",
    "print(categoricals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = data_types[data_types['continuous']==True].index.tolist()\n",
    "print(\"**first 10 continuous features**\")\n",
    "print(continuous[:10])\n",
    "print(\"**total continuous features**\")\n",
    "print(len(continuous))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless = data_types[data_types['useless']==True].index.tolist()\n",
    "print(\"**useless features**\")\n",
    "print(useless)\n",
    "print(\"**total useless features**\")\n",
    "print(len(useless))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test-validate split of labelled data for parameters tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**define target** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'diabetes_mellitus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**split data: train-validate-test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first split\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "for train_index, test_index in split.split(training, training[target]):\n",
    "    strat_train_set = training.loc[train_index]\n",
    "    strat_test_set = training.loc[test_index]\n",
    "training['test'] = np.where(training['encounter_id'].isin(strat_test_set['encounter_id']), True, False)\n",
    "non_test = training.loc[training['test'] == False]\n",
    "test = training.loc[training['test'] == True]\n",
    "\n",
    "# second split\n",
    "train, validate = train_test_split(non_test, test_size=0.2, stratify=non_test[target], random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train : ' + str(train.shape))\n",
    "print('test : ' + str(test.shape))\n",
    "print('validate : ' + str(validate.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**handle missing values by the split above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill NA of numerics with median value\n",
    "for col in continuous:\n",
    "    train[col].fillna(train[col].median(), inplace=True)\n",
    "    test[col].fillna(test[col].median(), inplace=True)\n",
    "    validate[col].fillna(validate.median(), inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill NAs of categoricals with most common values\n",
    "categorical_nas = ['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source']\n",
    "train[categorical_nas] = train[categorical_nas].fillna(train.mode().iloc[0])\n",
    "test[categorical_nas] = test[categorical_nas].fillna(test.mode().iloc[0])\n",
    "validate[categorical_nas] = validate[categorical_nas].fillna(validate.mode().iloc[0])\n",
    "\n",
    "print('unique values')\n",
    "for col in categoricals:\n",
    "    print(col + ' : ' + str(train[col].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals.remove(target)\n",
    "categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous.remove('encounter_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = continuous + categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[continuous]\n",
    "y_train = train[target]\n",
    "X_test = test[continuous]\n",
    "y_test = test[target]\n",
    "X_validate = validate[continuous]\n",
    "y_validate = validate[target]\n",
    "print('train')\n",
    "print(X_train.shape)\n",
    "print(y_train.value_counts())\n",
    "print('test')\n",
    "print(X_test.shape)\n",
    "print(y_test.value_counts())\n",
    "print('validate')\n",
    "print(X_validate.shape)\n",
    "print(y_validate.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning - hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = {'model':lgb.LGBMClassifier, 'param': {\n",
    "    'class_weight': {0:1, 1:hp.uniform('class_weight_1', 90, 450)},\n",
    "    'min_sum_hessian_in_leaf': hp.uniform('min_sum_hessian_in_leaf', 0.0, 1.0),\n",
    "    'max_bin': hp.choice('max_bin', np.arange(50, 750, 25, dtype=int)),\n",
    "    'num_leaves': hp.choice('num_leaves', np.arange(4, 256, dtype=int)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'subsample_for_bin': hp.choice('subsample_for_bin', np.arange(10000, X_train.shape[0], dtype=int)),\n",
    "    'min_child_samples': hp.choice('min_child_samples', np.arange(20, 500,5, dtype=int)),\n",
    "    'is_unbalance': hp.choice('is_unbalance', np.array([True, False], dtype = bool)), \n",
    "    'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n",
    "    'feature_fraction': hp.uniform('feature_fraction', 1/X_train.shape[1], 1.0),        \n",
    "    'max_depth': hp.choice('max_depth', np.arange(5, 12,1, dtype=int)),    \n",
    "    'lambda_l1': hp.uniform('lambda_l1', 0.0, 10.0),\n",
    "    'lambda_l2': hp.uniform('lambda_l2', 0.0, 10.0),\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction',1/X_train.shape[0]*10,1.0),\n",
    "    'bagging_freq': hp.choice('bagging_freq', np.arange(1, 11,1, dtype=int)),\n",
    "    'objective' : 'binary',\n",
    "    'boost_from_average': False ,\n",
    "    'boosting_type': hp.choice('boosting_type', np.array(['gbdt', 'dart'], dtype=str)),\n",
    "    'n_estimators' : hp.choice('n_estimators', np.arange(200, 5000, 50, dtype=int))\n",
    "}}   \n",
    "\n",
    "tested_models =[]\n",
    "tested_models.append(grid_params)\n",
    "  \n",
    "hp_space = hp.choice('classifier',tested_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "max_trials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_set = {}\n",
    "loss_list = []\n",
    "\n",
    "def objective(params):\n",
    "    model = params['model'](**params['param'])\n",
    "    ## fit model\n",
    "    model.fit(X_train, y_train)    \n",
    "    \n",
    "    ## predict\n",
    "    pred_test = model.predict(X_test) # class prediction\n",
    "#     pred_test = pd.DataFrame(model.predict_proba(X_test))#.iloc[:, 1] # probability prediction    \n",
    "   \n",
    "    ## evaluate predictions, change score if needed\n",
    "#     score = roc_auc_score(y_test, pred_test.iloc[:,1])  \n",
    "#     score = precision_score(y_true=y_test, y_pred=pred_test)\n",
    "    score = f1_score(y_true=y_test, y_pred=pred_test)\n",
    "    \n",
    "    ## define loss\n",
    "    loss = 1-np.round((score), decimals = 6) \n",
    "    hyperparameter_set[loss] = params\n",
    "    \n",
    "    print('Loss = ' + str(loss) + '\\n')\n",
    "    loss_list.append(loss)\n",
    "\n",
    "    return {'loss': loss, 'params': params, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "best = fmin(fn = objective, \n",
    "            space = hp_space, \n",
    "            algo = tpe.suggest, \n",
    "            max_evals = max_trials, \n",
    "            trials = trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the best model with the best hyparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import space_eval\n",
    "best_params_hyperopt = space_eval(hp_space, best)['param']\n",
    "best_params_hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model on train+test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [X_train, X_test]\n",
    "X_train_test = pd.concat(frames)\n",
    "frames = [y_train, y_test]\n",
    "y_train_test = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = lgb.LGBMClassifier( \n",
    "  bagging_fraction = 0.9278437351065486,\n",
    "  bagging_freq = 9,\n",
    "  boost_from_average = False,\n",
    "  boosting_type =  'gbdt' ,\n",
    "  class_weight = {0:1, 1:415.5324798320063},\n",
    "  colsample_bytree = 0.6616924071455909,\n",
    "  feature_fraction = 0.5057202614187002,\n",
    "  is_unbalance = False,\n",
    "  lambda_l1 = 1.1035610361541048,\n",
    "  lambda_l2 = 9.287119306850947,\n",
    "  learning_rate = 0.1984964279262592,\n",
    "  max_bin = 350,\n",
    "  max_depth = 10,\n",
    "  min_child_samples = 480,\n",
    "  min_sum_hessian_in_leaf = 0.9322887572635592,\n",
    "  n_estimators = 3150,\n",
    "  num_leaves = 55,\n",
    "  objective =  'binary',\n",
    "  subsample_for_bin = 48216)\n",
    "\n",
    "model.fit(X_train_test, np.ravel(y_train_test), eval_set = (X_validate, np.ravel(y_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation on X_train_test/X_validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train =  model.predict_proba(X_train_test)[:, 1]\n",
    "predictions_valid =  model.predict_proba(X_validate)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test =  model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "sns.set(rc={'figure.figsize': (20, 10)})\n",
    "\n",
    "## cm train+test set\n",
    "y_pred_train = model.predict(X_train_test)\n",
    "cm = confusion_matrix(y_train_test, y_pred_train)\n",
    "# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# ax1 = plt.subplot(1, 2, 1)\n",
    "# sns.heatmap(cm, annot=True, ax=ax1, fmt='.0f', cmap='magma')\n",
    "# #annot=True to annotate cells\n",
    "\n",
    "# # labels, title and ticks\n",
    "# ax1.set_xlabel('Predicted labels')\n",
    "# ax1.set_ylabel('True labels')\n",
    "# ax1.set_title('Confusion Matrix Training')\n",
    "# ax1.xaxis.set_ticklabels(['No diabetes', 'Diabetus melitus'])\n",
    "# ax1.yaxis.set_ticklabels(['No diabetes', 'Diabetus melitus'])\n",
    "\n",
    "\n",
    "ax2 = plt.subplot(1, 2, 1)\n",
    "sns.heatmap(cm, annot=True, ax=ax2, fmt='.2f', cmap='viridis')\n",
    "#annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax2.set_xlabel('Predicted labels')\n",
    "ax2.set_ylabel('True labels')\n",
    "ax2.set_title('Confusion Matrix Training set', size = 14)\n",
    "ax2.xaxis.set_ticklabels(['No diabetes', 'Diabetus melitus'])\n",
    "ax2.yaxis.set_ticklabels(['No diabetes', 'Diabetus melitus'])\n",
    "\n",
    "\n",
    "## cm validation set\n",
    "y_pred_validate = model.predict(X_validate)\n",
    "cm = confusion_matrix(y_validate, y_pred_validate)\n",
    "# cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "ax1 = plt.subplot(1, 2, 2)\n",
    "sns.heatmap(cm, annot=True, ax=ax1, fmt='.0f', cmap='magma')\n",
    "#annot=True to annotate cells\n",
    "\n",
    "# labels, title and ticks\n",
    "ax1.set_xlabel('Predicted labels')\n",
    "ax1.set_ylabel('True labels')\n",
    "ax1.set_title('Confusion Matrix Validation set', size = 14)\n",
    "ax1.xaxis.set_ticklabels(['No diabetes', 'Diabetus melitus'])\n",
    "ax1.yaxis.set_ticklabels(['No diabetes', 'Diabetus melitus'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "metrics.plot_roc_curve(model, X_validate, y_validate)\n",
    "plt.title('ROC curve lightGBM model', fontsize=14, weight=\"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_validate, model.predict(X_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lgb.plot_importance(model, figsize=(30, 30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [X_train, X_test, X_validate]\n",
    "X = pd.concat(frames)\n",
    "frames = [y_train, y_test, y_validate]\n",
    "y = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled[continuous].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probs = pd.DataFrame(model.predict_proba(unlabeled[continuous])[:, 1])\n",
    "# predicted_labels = pd.DataFrame(model.predict(unlabeled[model_features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = unlabeled['encounter_id'].to_frame()\n",
    "result[target] = predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('/home/jovyan/work/analysis/DATASCI-WiDS/submissions/submission_lgb_hyperopt_19022021.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c widsdatathon2021 -f '/home/jovyan/work/analysis/DATASCI-WiDS/submissions/submission_lgb_hyperopt_19022021.csv' -m \"lightGBM hyperopt nums\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"code.\"></form>''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}